{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some nessary libraries\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from __future__ import unicode_literals\n",
    "from hazm import Normalizer, Lemmatizer\n",
    "from hazm import sent_tokenize, word_tokenize\n",
    "from hazm import Lemmatizer\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df = pd.read_table('emails.csv', header=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham\n"
     ]
    }
   ],
   "source": [
    "# cleaning data\n",
    "lables = []\n",
    "for email in df[0]:\n",
    "  lables.append(email.split(',')[-1])\n",
    "lables.remove(lables[0])\n",
    "print(lables[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text,label'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = df[0].copy()\n",
    "for i in range(len(text)):\n",
    "  if i == 0:\n",
    "    continue\n",
    "  text[i] = text[i].split(',')[:-1][0]\n",
    "text.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding lables\n",
    "encoder = LabelEncoder()\n",
    "Y = encoder.fit_transform(lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning text\n",
    "processed = text.str.replace(r'^.+@[^\\.].*\\.[a-z]{2,}$',\n",
    "                                 ' emailaddress ',regex=True)\n",
    "processed = processed.str.replace(r'(http|ftp|https):\\/\\/[\\w\\-_]+(\\.[\\w\\-_]+)+([\\w\\-\\.,@?^=%&amp;:/~\\+#]*[\\w\\-\\@?^=%&amp;/~\\+#])?',\n",
    "                                  ' webaddress ',regex=True)\n",
    "processed = processed.str.replace(r'تومان|ریال|دلار|\\$|تومن|هزار|میلیون|یورو', ' moneysymb ',regex=True)\n",
    "\n",
    "processed = processed.str.replace(r'^\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$',\n",
    "                                  ' phonenumbr ',regex=True)\n",
    "processed = processed.str.replace(r'\\d+(\\.\\d+)?', ' numbr ',regex=True)\n",
    "\n",
    "processed = processed.str.replace(r'[^\\w\\d\\s]', ' ',regex=True)\n",
    "\n",
    "processed = processed.str.replace(r'\\s+', ' ',regex=True)\n",
    "\n",
    "processed = processed.str.replace(r'^\\s+|\\s+?$', '',regex=True)\n",
    "\n",
    "processed = processed.str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization\n",
    "normalizer = Normalizer()\n",
    "for i in range(len(processed)):\n",
    "  if i == 0:\n",
    "    continue\n",
    "  processed[i] = normalizer.normalize(processed[i])\n",
    "  processed[i] = word_tokenize(processed[i])\n",
    "from hazm.utils import stopwords_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop words\n",
    "stopwords = set(stopwords_list())\n",
    "processed = processed.apply(lambda x:[\n",
    "    term for term in x if term not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization\n",
    "ps = Lemmatizer()\n",
    "processed = processed.apply(lambda x: [ps.lemmatize(term) for term in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for i in processed:\n",
    "  for j in i:\n",
    "    all_words.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 15108\n",
      "Most common words: [('numbr', 6142), ('webaddress', 2864), ('moneysymb', 1319), ('داشت#دار', 891), ('قیمت', 837), ('خرید', 817), ('آموزش', 757), ('کرد#کن', 678), ('پست', 662), ('سایت', 584), ('استفاده', 564), ('شد#شو', 525), ('کار', 494), ('دانشگاه', 493), ('صورت', 484)]\n"
     ]
    }
   ],
   "source": [
    "all_words = nltk.FreqDist(all_words)\n",
    "print('Number of words: {}'.format(len(all_words)))\n",
    "print('Most common words: {}'.format(all_words.most_common(15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features = list(all_words.keys())[:1500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(message):\n",
    "    words = message\n",
    "    features = []\n",
    "    for word in word_features:\n",
    "        features.append(int(word in words))\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [find_features(text) for text in processed.iloc[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X,Xt, Y,Yt = model_selection.train_test_split(featuresets,Y[1:], test_size = 0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Xt, Y, Yt = np.array(X), np.array(Xt), np.array(Y), np.array(Yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier: 0.736\n",
      "DecisionTreeClassifier: 0.8\n",
      "RandomForestClassifier: 0.944\n",
      "LogisticRegression: 0.96\n",
      "SGDClassifier: 0.948\n",
      "MultinomialNB: 0.94\n",
      "SVC: 0.936\n"
     ]
    }
   ],
   "source": [
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    LogisticRegression(),\n",
    "    SGDClassifier(),\n",
    "    MultinomialNB(),\n",
    "    SVC()\n",
    "]\n",
    "for clf in classifiers:\n",
    "    clf.fit(X, Y)\n",
    "    y_pred = clf.predict(Xt)\n",
    "    print(clf.__class__.__name__ + ':', accuracy_score(Yt, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
